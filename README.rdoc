=Pre-assembly

This is a Ruby implementation of services needed to prepare objects to be assembled and then accessioned into the SUL digital library.

Uses ruby 1.9.3

==Releases

1.0.0::  Released to production
1.2.0::  Changed <provider_checksum> node to regular <checksum> node.  Must be run in combination with assembly 1.2.0.
1.2.2::  Significant refactoring of cleanup scripts and movement of methods to a new Util class.  Update integration tests.
1.3.0::  Add the ability to re-accession
1.3.1::  Bug fixes related to re-accessioning
1.3.2::  Bug fixes related to smpl content metadata processing
1.3.3::  Add the ability to leave the progress log filename as nil in the config file, and it will be created automatically.
1.3.4::  Add contentMetadata and resourceTypes of 'image' during pre-assembly if you specify :simple_image as project type.
1.4.6::  Add functionality to discovery_report to provide information on whether objects are registered and if they have APOs
1.4.7::  Allow "book_as_image" as new project content type.
1.4.8::  Fix workflow_status report so it doesn't crash if a workflow is not found
1.4.9::  Bug fix in cleanup method
1.5.0::  Add the ability to accession or re-accession only specific items via "except" or "only" parameters
1.5.1::  Updates to discovery report to only show objects that will be processed
1.5.2::  Add some utility methods to quickly update datastreams for a list of DOR objects
1.5.3::  Add more options to the discovery_report to make it useful when running manifest style projects
1.5.4::  bug fixes to discovery report
1.5.5::  Add some more reporting info to the discovery_report (to tell you how many problems were found when running)
1.5.6::  Added additional check to discovery report -- check for any files that are present in the bundle directory but not referenced in the manifest
1.5.7::  Add the ability to check the uniqueness of source IDs to the discovery report
1.5.8::  bug fixes, update to fix tests
1.5.9::  update generate_collection_report to add more columns
1.6.0::  rename generate_collection_report to project_tag_report and add a new report that uses the progress YAML file to only report on successfully accessioned druids
1.6.1::  allow users to specify a manifest file for descriptive metadata purposes even if should_register=false; allow users to bypass image validation
1.7.0::  allow for 'joined' method of content metadata generation; allow user to specify publish/preserve/shelve attributes by mime-type
1.7.1::  add further validation checks to ensure only compatible values are set in YAML before proceeding, some refactoring to keep bundle class smaller, remove 'use_druid_minter' parameter and set it to a new style of getting druids, show warnings if you set development only parameters
1.7.2::  set completion_report to check actual workflow states in DOR instead of relying on SOLR, which can be out of date
1.7.3::  small bug fix when checking writable status of directories and files; try and find druids from barcodes when running discovery report
1.7.4::  more validation fixes, to indicate that container barcode projects must have the APO DRUID set
1.7.5::  add another check to discovery_report which confirms readable file permissions on object files
1.7.6::  bug fix in discovery_report for barcode projects
1.7.7::  move the Utils class into it's own gem so it can be used elsewhere ... methods that were PreAssembly::Utils are now Assembly::Utils; refactoring to account for new DruidTools gem usage
1.7.8::  bug fixes; update how configuration is done to make it part of Assembly-utils gem
1.7.9::  add time stamps to progress display on screen (or into nohup.out file)
1.8.0::  allow the web service call to retry 5 times, sleeping between each call in order to avoid pre-assembly crashing if the dor web service is temporarily unavailable
1.8.1::  refactor: adjust object_file class to subclass from Assembly::ObjectFile so we don't need to redeclare convenience methods
1.9.0::  use the content metadata generation methods in the updated Assembly::ObjectFile gem, which expands the possible types of metadata that can be generated, update templates and documentation
1.9.1::  use the new DruidTools gem to generate staged content in the druid tree format: /oo/000/oo/0001/oo000oo0001/content
1.9.2::  update how set relationships are stored in an object -- add both an isMemberOf and an isMemberOfRelationship
1.9.3::  small updates to force new usage of assembly-objectfile gem for latest contentmetadata generation; additional output at end of preassembly
1.9.4::  update discovery_report to include checks for APO existence and existence of assemblyWF workflow definition
1.9.5::  more additions to discovery_report to provide total size and count by mimetype of discovered files; eliminate need to directly compute md5; this is now done via the assembly-objectfile gem
1.9.6::  add a new configuration parameter that determines if items are staged using the new style of druid trees or the old style of druid trees in the dor workspace
1.9.7::  provide the option to have no contentMetadata.xml file generated during pre-assembly.  Useful if you have a current custom created one ready to stage.
1.9.8::  update web service call to remove "v1" from URL
1.9.9::  allow the content metadata creation style to be discovered from already registered objects by inspecting the 'Process : Content Type' tag.  The default is used if not found or mapping incomplete.
1.10.0:: added support for the dir_validator gem
1.10.1:: updated discovery report to fetch druids; add files needed for mjf
1.10.2:: update discovery report to optionally show all staged files; add new parameter to stageable discovery to optionally stage files only
1.10.3:: show detailed exception messages if initiate_accessioning_workflow step fails after several attempts (useful for debugging)
1.10.4:: update discovery report
1.10.5:: update README to indicate new convenience methods available in assembly-utils, update required version of assembly-utils
1.10.6:: Update service_root configuration to allow different service URLs (e.g. with or without v1) for test and production
1.10.7:: Allow the dor register call to retry 5 (configurable) times, sleeping between each call in order to avoid pre-assembly crashing if the dor web service is temporarily unavailable
1.10.8:: Add a new parameter to allow pre-assembly to be throttled (add a sleep time between objects)
1.10.9:: Catch errors on register_object_in_set method call, and allow for retries before raising an exception
1.10.10:: Add spaces in filenames checks to discovery report
1.10.11:: Catch errors on get_pid_from_suri method call, and allow for retries before raising an exception
1.10.12:: Delete assemblyWF and accessionWF workflows from objects when re-accessioning using updated assembly-utils gem  
1.11.1::  Bump up to latest versions of assembly gems; fix bug in publish attributing adding during content metadata generation
1.11.2::  If user had a trailing slash on the bundle_dir parameter, object discovery might fail.  Remove any trailing slashes during setup.
1.11.3::  Make SMPL contentMetadata generation more robust so it doesn't crash if the checksum is missing from the preContentMetadata file.
1.11.4:: Changed the object type and resource type from 'file' to 'media' for SMPL conntent metadata. 
2.0.0::  Allow user to stage items via symlink instead of copy -- useful when source bundle directory contains large files.  New YAML config parameter is called "staging_style" and can be set to "copy" or "symlink" (default is "copy").
2.0.1::  Attempt cleanup if registration step fails -- try to delete object and purge from solr before trying again
2.0.2:: Add some more intelligence to the validate_usage step to better indicate errors in YAML configuration; default to new druid tree
2.0.3:: Verify that pid starts with druid: when initializing the assembly workflow
2.0.4:: Restructure project specific folder to be more generalizable.  Add new "revs" project specific file that does LC term lookups when generating MODs.
2.0.5:: Add new "revs" project specific files with country and state parsing for location nodes on MODs.
2.0.6:: Add some new "revs" project specific methods to check format field
2.1.0:: Add a basic remediation framework for mass remediating objects.
2.1.1 - 2.1.3:: More updates to remediation framework; revs metadata and image updating specific code
2.1.4:: Allow DOR items to be registered even when a label is not provided in the manifest, by using a default label instead
2.1.5:: If registration fails, try to delete object by source ID as well as by druid
2.1.6:: Update remediation to allow you to skip workflow checks for in accessioning and versioning required when testing in development and production.
2.1.7:: Update remediation script to allow you to pass in a pre-assembly log file as well as a CSV with druids
2.1.8:: Show total number of objects and remaining in remediation output
2.2.0:: Allow the user to specify multiple sets/collection objects that should be associated with an item when registering
2.2.1:: Adjust how validate_files works to also check for color profile since that check is no longer provided with the existing method
2.2.2:: Change default mapping for flipbook registered object to simple_book content metadata; don't override yaml content metedata type with registered object content tag unless user explicitly indicates this should be done
2.2.3:: Move SMPL preContentMetadata generation methods into a class and allow it to be run automatically whenever SMPL accessioning occurs
2.2.4:: Make SMPL content metadata generation per object so that you do not need to run an extra step ahead of pre-assembly
2.2.5:: Make SMPL searching of checksums in MD5 file case insensitive for the role folder
2.2.6:: Add some additional info to the discovery report for SMPL manifest projects 
2.2.7-2.2.8:: Enable automated combination of all techmd.xml files for SMPL projects into a single technical metadata datastream; cleanup old methods
2.3.1:: Remove revs specific shared methods from revs project file, include their functionality from the new revs-utils gem
2.3.2:: Small changes to discovery report around checking for APOs and objects; do not fail if label column is missing from manifest for should_register=false
2.3.6:: Use latest version of assembly-image and assembly-objectfile gems
2.3.7:: Use latest version of revs-utils.  Allow manifests to have UTF-8 characters (switch manifest loading from csv-mapper to standard CSV library)
2.3.8:: Fixes to manifest loading and access to always use hash style references for columns (instead of allowing for struct like syntax that came from csv-mapper). Fix rdf gem to 1.0.9 until activefedora is updated to v6
2.3.9:: Send default title to registration service if no label is provided in the manifest
2.4.0:: Added a new MODs generation report so you can test your MODs desc md template before you actually run accessioning.

To check which version is running, cd into the home directory of the project and look at the VERSION file:

  cd /home/lyberadmin/pre-assembly/current
  more VERSION

==Project Location

The code is deployed onto sul-lyberservices-test and sul-lyberservices-prod in the <tt>/home/lyberadmin/pre-assembly</tt> with the latest version in the "current" subdirectory.

==Running

1. Gather information about your project, including:
   * The location of the materials.  You will need read access to this location from the servers you will be accessioning in (e.g. test and production).
   * Whether the objects are already registered or not.
   * The location of any descriptive metadata.
   * Whether you will be flattening the folder structure of each object when accessioning (e.g. discarding any folder structure provided to you in each object).
   * The DRUID of the project's APO.
   * The DRUID of the set object you will be associating your objects with (if any).
   * If your objects are not yet registered and you have a manifest file in CSV format, make sure you have columns for sourceid, filename, and label.  See config/projects/manifest_template/TEMPLATE_manifest.csv for an example manifest.  See the "manifest" section below for more information.
   * If you are using a manifest file in CSV format and want to create descriptive metadata, create a MODs XML template.  See the "descriptive metadata" section below for more details.

2. Create a project-configuration YAML file using the data you gathered above. Store this file in a location where it can be accessed by the server (test or production). You should create a YAML file for each environment specifying the parameters as appropriate. Use the convention of "projectname_environment.yaml", e.g. "revs_test.yaml". If you have multiple collections to associate your objects with, you will need to run in multiple batches with multiple YAML files. You can add your collection name to the end of each YAML filename to keep track (e.g. "revs_test_craig.yaml")

   The YAML file can be stored anywhere that is accessible to the server you are running the code on.
   However, for simplicity, we recommend you store the YAML at the root of your bundle directory, or create
   a new project folder, place your YAML file into it and then place your bundle directory into your new
   project folder. PLEASE DO NOT PLACE YOUR YAML FILE INTO THE PRE-ASSEMBLY DIRECTORY ITSELF ANYWHERE ON THE
   SERVER. IT WILL BECOME HARD TO FIND AND BE SUBJECT TO DELETION WHEN NEW CODE IS DEPLOYED.

   Example:

   * Your content is on /thumpers/dpgthumper-staing/Hummel
   * Create a YAML file at /thumpers/dpgthumper-staging/Hummel/hummel_test.yaml
   * Move your content (if you can) into /thumpers/dpgthumper-staging/Hummel/content

   If you cannot move your content, be sure your YAML bundle discovery glob and/or regex are specific enough
   to correctly ignore your YAML file during discovery. Or, alternatively, place your YAML file in a
   location other than the bundle.

   * See config/projects/TEMPLATE.yaml for a fully documented example of a configuration file.
   * See config/projects/manifest_noreg_example.yaml for a specific example using a manifest.
   * See config/projects/reg_example.yaml for a specific example using a file system crawl.

3. Check the permissions on the bundle directory, iteratively. You need read permissions on all the bundle directory folders and files. You need to have write permissions in the location you plan to write the log file too (often this cannot be the thumper drives since it is mounted as read-only).

4. You may benefit from running some objects in a local or test environment. If your objects are already registered, this may require pre-registering a sample set in test as well as production using the same DRUIDs that are identified with your content. You may also have to move a small batch of test content to a location that is visible to sul-lyberservices-test. Since the thumper drives are not mounted on the test server, you can use the /dor/content mount on test for this purpose.

5. Make sure you have an APO for your object, and that the administrativeMetadata data stream has the <tt><assemblyWF></tt> defined in it. If it does not, go to https://consul.stanford.edu/display/APO/Home and find the "Current FoXML APO template" link at the bottom of the page. Download and open the template, find the <tt><assembly></tt> node and copy it. Go to Fedora admin for each relevant environment (test/production) and this node to the administrativeMetadata stream. If you don't have this workflow defined in your APO, then the assembly robots will never operate and accessioning will not operate. This APO should be defined using the same DRUID in test and production if you intend to run in both locations.

6. You can perform a dry discovery run to test your YAML configuration. This run will enumerate the discovered objects, tell you how many files were discovered in each object, check for filename uniqueness in each object, and confirm objects are registered with an APO (for projects where objects are pre-registered). This dry run is particularly important if you are flattening each object's folder structure during pre-assembly (e.g. each object has images in a '00' and '05' directory, but you don't want to retain those folders when accessioning), since you will want to check to make sure each file in a given object has unique filenames. For projects that use manifests for object discovery along with checksum files, you can optionally have checksums computed and confirmed. This is really only useful if you are staging content and not accessioning immediately (since the accessioning process will reconfirm checksums).

   First log into sul-lyberservices-test or prod as needed, and then cd into the pre-assembly directory, e.g.

     ssh lyberadmin@sul-lyberadmin-test.stanford.edu
     cd pre-assembly/current
     ROBOT_ENVIRONMENT=test bin/discovery_report YAML_FILE

   You will probably want to run this against a specific environment so it can connect to DOR and confirm registration on the appropriate server, e.g:

     ROBOT_ENVIRONMENT=production bin/discovery_report YAML_FILE
  
   You will see a report containing:
   * the total number of objects discovered
   * the names of each discovered object along with the number of files which will be discovered in that object
   * any entries (directories or files) in the bundle directory which will *not* be discovered based on your configuration.
   * the total number and listing of any objects which have duplicate filenames.  You must resolve the duplicate filenames if you intend to flatten the folder structure when accessioning.
   * for manifest style projects, the label and source id along with if all source IDs contained in the manifest are unique 
   * for manifest style projects, a listing of any folders/files present in the bundle directory that are not referenced in the manifest... some will be expected (such as a checksum file), but this will let you see if any expected images/data are missing from the manifest
   * for SMPL style projects, a listing of the the number of files found in the content metadata manifest ... which will let you know if you it has correctly found the object in the smpl_manifest.csv file -- a 0 would mean none were found or listed, which is a problem
   
   If any errors occur, they will be displayed and a total error count is shown at the bottom.

   To send the report to a CSV file for better sorting and viewing in Excel, send the output to a file using
   normal UNIX syntax, e.g.:

     ROBOT_ENVIRONMENT=production bin/discovery_report YAML_FILE > /full/path/to/report/filename.csv

   When sending output to a CSV, you will not see any terminal output while the report is running.

   Options for discovery report: You can add the following parameters after the YAML_FILE name. Note that
   adding each option may make the report time consuming, especially for large number of objects. Some
   options only work for certain styles of projects.

   +confirm_checksums+:: for manifest style projects, will compute and confirm checksums against the checksum file if it exists -- useful it you are not accessioning immediately
   +check_sourceids+::  for manifest style projects, will confirm source IDs are globally unique in DOR (sources ids area already checked for local uniqueness in the manifest)
   +no_check_reg+:: for projects where objects are to be registered, DONT'T check if objects are registered and have APOs (assuming they are supposed to be registered already)
   +show_staged+:: will show all files that will be staged (warning: will produce a lot of output if you have lots of objects with lots of files!)
   +show_smpl_cm+:: will show content metadata that will be generated for each SMPL object using the supplied manifest (warning: will produce a lot of XML output if you have lots of objects with lots of files!)

   e.g.

     ROBOT_ENVIRONMENT=production bin/discovery_report YAML_FILE confirm_checksums check_sourceids > report.csv


7. To run pre-assembly locally:

     # Normal run.  Will restart and crete a new log file, overwriting any existing log file for that project.
     bin/pre-assemble YAML_FILE

     # Run in resume mode, which will automatically pick up where left off based on the log file.
     bin/pre-assemble YAML_FILE --resume

   Again, you can add ROBOT_ENVIRONMENT=XXXX to the beginning of the command to run in test, production or
   other modes as needed.

8. Running in the production environment:

   - Navigate to the production box, in the pre-assembly area.
   - Set the ROBOT_ENVIRONMENT=production.
   - Run pre-assembly with nohup and in the background (&).
   - Optionally, include the <tt>--resume</tt> option.
  
   See the example below:

     ssh lyberadmin@sul-lyberservices-prod.stanford.edu
     cd /home/lyberadmin/pre-assembly/current
     ROBOT_ENVIRONMENT=production nohup bin/pre-assemble YAML_FILE &

   If you want to run multiple nohup jobs simultaneously, you can redirect screen output to a different log file:
  
     ROBOT_ENVIRONMENT=production nohup bin/pre-assemble YAML_FILE > another_nohup_filename.out 2>&1&
  
   Various ways to monitor progress:
   1. The workflow grid in Argo, using your project tag to filter.
   2. grep pid PROGRESS_LOG          # Using the filename defined in YAML progress_log_file.
   3. tail -999f log/production.log  # Detailed logging info for the pre-assembly project itself.
   4. tail -999f nohup.out           # Errors, etc from unix output (or "another_nohup_filename.out" in the example above)

   Be sure to keep your progress log file somewhere useful and be aware if you restart pre-assembly without
   using the <tt>--resume</tt> switch, it will be overwritten. You will need the progress log for cleanup
   and restarting.

== Expert Cheatsheet

Here's a quick summary of the basic execution steps:

 # Goto VM and select environment
 % ssh lyberadmin@lyberservices-prod
 % export ROBOT_ENVIRONMENT=production 

 # Goto project directory and copy YAML and related files
 % cd MyProject
 % YAML=MyProject_$ROBOT_ENVIRONMENT.yaml
 % BUNDLEDIR=`grep bundle_dir: $YAML | cut -d \' -f2`
 % cp $YAML $BUNDLEDIR
 # optionally copy other required files to $BUNDLEDIR

 # Execute the pre-assembly
 % cd $HOME/pre-assembly/current
 % bin/discovery_report $BUNDLEDIR/$YAML
 % bin/pre-assemble $BUNDLEDIR/$YAML

=Legacy Project Notes

The assembly robots will automatically create jp2 derivates from any TIFFs, JP2s, or JPEGs. If you are
working on a legacy project that has JP2s already that were generated from source TIFFs, you should *not*
stage those files during pre-assembly, or else you will end up with two JP2s for each TIFF. You can do this
by using a regex to exclude .JP2 files or by only staging certain subfolders. If you do stage the JP2 files
and they have the same filename as the TIFF (but with a different extension) they will be kept as is (i.e.
they will NOT have JP2s re-generated from the source TIFFs). If you do stage the JP2 files and they have a
different basename than the TIFFs, they WILL be re-generated, and you will end up with two copies, in two
different resources.

==Deployment

  cap development deploy  # for lyberservices-dev
  cap testing deploy  # for lyberservices-test
  cap production deploy # for lyberservices-production

Enter the branch or tag you want deployed.

See the Capfile for more info

==Setting up code for local development

    # Clone project.
    git clone `whoami`@corn.stanford.edu:/afs/ir/dev/dlss/git/lyberteam/pre-assembly.git
    cd pre-assembly

    Copy the default configs and use them for local and test.

      cp config/environments/test.example.rb config/environments/test.rb
      cp config/environments/local.example.rb config/environments/local.rb

    You will need to have a certificate to access DOR-test in order to run integration tests from your laptop.  The certificates are placed
    in your laptops' "config/certs" folder and are referenced in the "config/environments/test.rb" file.  Talk to DevOps to get a certificate.

    # Get needed gems.
    bundle install

    # Confirm that it's working by running the tests as decribed below.

    To make your life easier, it's easiest to put this in your bash profile so you don't need to identify each time you run a command on your laptop during development:

      export ROBOT_ENVIRONMENT=local


==Running tests

To run all tests, use the command below.  Note that to run integration tests, you will need to be connected to the VPN (even if on campus) and with a SSH tunnel connection to sul-solr (see below)

    bin/run_all_tests

To run the unit tests (fast) and the integration tests (slower) separately, use the commands below.  As noted above, integration tests require VPN and an SSH tunnel (see below)
    
    bundle exec rspec spec        # no DOR access required, no VPN
    bundle exec rspec integration # DOR access required, VPN, SSH tunneling required

To create an SSH tunnel to run the integration tests, you will first need to be on VPNed in (even if on campus). Then, in a separate terminal window: 

  ssh sul-solr-a -L 8080:localhost:8080

Leave that terminal window open as long as you need to run integration tests.

See this page for more information: https://consul.stanford.edu/display/DLSSINFRAAPP/SUL+Solr+Infrastructure

==Environments

Use the ROBOT_ENVIRONMENT=xxxxx in front of commands to run in a specific environment.  Current available environments are:

local:: your laptop
development:: development servers
test:: test servers
production:: production servers

The server environments define which instance of DOR is connected to, as well as the workflow and other
services. If you run in the incorrect environment, you will find your objects registered in unexpected
places, or you may run into errors when objects you believe should be registered are not.


==Screen Command

If screen is installed on the server you are using (currently not in production), another possibility instead of running nohup is to run 
using the "screen" command.  (NOTE: currently, screen is not available in production).

Start a new screen by typing:

 $  screen

You can then start pre-assembly without nohup, just like you would locally:

 $  ROBOT_ENVIRONMENT=production bin/pre-assemble YAML_FILE

You can then detach from the screen by pressing ctrl-a, ctrl-d and then exit from the server.

You can come back to your screen by re-logging into the server, and typing 

 $ screen -r

You can also see a list of available screens by typing

 $ screen -list

For more info on screen, see http://kb.iu.edu/data/acuy.html


==Troubleshooting

1. If you don't see all of your objects being discovered or no files are found in discovered objects, check the permissions on the bundle directory. You need read permissions on all the bundle directory folders and files.

2. Be sure you are running on the correct server in the correct environment.  See the "environment" section above.

3. Be sure you have read access to the YAML file you created from the server you are running on.

4. Be sure you have write access to the location you have specified for your progress log file. When running as lyberadmin on the test and production machines, you will NOT have write access to the thumper drivers. You should store your progress log file elsewhere, such as /dor/preassembly

5. Check to see if the assembly and accessioning robots are running in the environment you are using. See the "Starting Robots" section below. It is not recommended that you start robots in production without consulting the Lyberstructure team.

6. If you don't see JP2s being created (or recreated) for your content, this is probably due to one of the following problems:

   a. The content metadata generated by pre-assembly didn't set a resource type or set a resource type other than "image" or "page". Assembly will only create jp2s for images containing in resources marked as "image" or "page". Pre-assembly will do this automatically for :simple_image and :simple_book projects, but check the output of the content metadata to be sure.

   b. The image was not a mimetype of 'image/jpeg' or 'image/tiff'.  Any other mimetype will be ignored.

   c. Your input image was corrupt or missing a color space profile.  This will usually cause the jp2-create robot to fail and throw an error in that workflow step.

   d. You had an existing JP2 in the directory that matched a tiff or jpeg.  In this case the jp2-create robot will not overwrite any existing files just to be safe.

   e. You had an existing JP2 in the directory that matched a DPG style filename (e.g. if you had existing tiff called xx000yy1111_00_01.tif and a jp2 called xx000yy1111_05_01.jp2), you will not get another jp2 from that tiff even though there would not be a filename clash, under the principle that it refers to the same image).

7. If 6b or 6c above, it is possible to spot check images to assess the problem:

     $ ssh lyberadmin@lyberservices-prod
     $ cd ~/pre-assembly/current
     $ ROBOT_ENVIRONMENT=production bin/console
     > a=Assembly::Image.new('/full/path/to/image')
     > a.jp2able? # (if "false" then diagnose the problem further)
     > a.exif['profiledescription'] # (if "nil" then it is missing color profile)
     > a.mimetype # (should be "image/tiff" or "image/jpeg")
     > a.width # should give you the image width
     > a.height # should give you image height

   It is possible to force add color profiles to a single image or all of the images in a given directory:

    source_img=Assembly::Image.new('/input/path_to_file.tif') # add to a single image
    source_img.add_exif_profile_description('Adobe RGB 1998')
 
   or

    Assembly::Images.batch_add_exif_profile_description('/full_path_to_tifs','Adobe RGB 1998')    # add to multiple images

8. If you see incorrect content metadata being generated, note that if should_register = false, the 'Process : Content Type' tag for each existing object will be examined. If a known type is set in this tag, it will be used to create content metadata instead of the default set in project_style[:content_structure]. Check the tag for each object if the style is not matching what you have set in the YAML configuration file. Also note that if <tt>content_md_creation[:style]</tt> is set to 'none', then no content metadata will be generated.

==Restarting a job

If you have failed objects during your pre-assembly, these will either cause pre-assembly to terminate immediately (if the failure is non-recoverable) or it will continue and log the errors. The progress log file you specified in your YAML configuration will contain information about which bundles failed. You can re-start pre-assembly and ask it to re-try the failed objects and continue with any other objects that it hadn't done yet. To do this, use the --resume flag when you run pre-assembly:

    ROBOT_ENVIRONMENT=production bin/pre-assemble YAML_FILE --resume

==Post Accessioning Reports

Two reports are available, but you should really use Argo for reporting at this point.

If you wish to use these reports, both produce the following output, but differ in how they locate objects
to report on. The output for both reports is a CSV file in the "log" folder of your checked out pre-assembly
code. Both will report on up to 50,000 rows and includes the following columns:

* druid
* label
* source_id
* dc:title
* published status
* shelved status
* PURL url 
* total files in object
* number of files by file extension

The first report is called a "project_tag_report" and includes ALL objects in DOR tagged with a specific
project tag. This is useful for a global project overview and is cumulative (i.e. as more objects are added
with that tag, the report will be bigger if run again).

 ROBOT_ENVIRONMENT=production bin/project_tag_report PROJECT_TAG

where PROJECT_TAG is the Argo project tag (e.g. "Revs"). If your project tag has spaces in it, be sure to
use quotes, like this:

 ROBOT_ENVIRONMENT=production bin/generate_collection_report "Stanford Oral History Project"

The second report is called a "completion_report" and uses the pre-assembly YAML configuration file to
determine the successfully accessioned objects. Only those objects are included in the report. The report is
run with:

 ROBOT_ENVIRONMENT=production bin/completion_report YAML_FILE

e.g.:

 ROBOT_ENVIRONMENT=production bin/completion_report /thumpers/dpgthumper2-smpl/SC1017_SOHP/sohp_prod_accession.yaml


==Manifests

Manifests are a way of indicating which objects you will be accessioning. A manifest file is a CSV, UTF-8
encoded file and works for projects which have one file per object (container = one file in this case), or
projects with many files per object (container = folder in this case).

There are a few columns required in the manifest depending on if should_register is true or false:

container:: container name (either filename or folder name) -- always required
sourceid:: source ID used when registering (required if should_register = true)
label:: label used when registering (required if should_register = true)
druid:: druid of object (required if should_register = false, used to identify which object the row refers to) - the druids should include the "druid:" prefix (e.g. "druid:oo000oo0001" instead of "oo000oo0001")

The first line of the manifest is a header and specifies the column names. Column names should not have
spaces and it is easiest if they are all lower case. These columns are used to register objects and indicate
which file goes with the object. If the container column specifies a filename, it should be relative to the
manifest file itself. You can have additional columns in your manifest which can be used to create
descriptive metadata for each object. See the section below for more details on how this works.

The actual names of the columns above (except for "druid") can be set in the YAML file.  The druid column must be called "druid".

A sample manifest file is located in config/projects/manifest_template/TEMPLATE_manifest.csv for an example.  

== Descriptive Metadata

If descriptive metadata is supplied in a source known to common accessioning (currently MDToolkit or Symphony), then no action is required during pre-assembly other than ensuring your DRUIDs and/or barcodes match the ones in MDToolkit or Symphony.

If you are supplying a manifest file instead of using object discovery via file system crawling, then you can also create a descriptive metadata MODs file for each object using content supplied in the manifest.  By creating a template XML MODS file, placing with your YAML configuration file and ensuring it's filename is indicated in your YAML configuration, you can tell pre-assembly to generate a MODs file per object.  The generated MODs file should be called "descMetadata.xml" and will be staged alongside the content.  This file is automatically picked up during common accessioning.  

The MODs file is generated by taking the XML template you supply, and filling in any <tt>[[field]]</tt> values in the template with the corresponding column from the manifest.

For example, if your template has

 <mods><title>[[description]]</title></mods>

and you have a column called "description" in your manifest and you have a row with a value of "picture of me", you will get that value filled into your template for that specific object:

 <mods><title>picture of me</title></mods>

In addition, the entire MODs template is passed through an ERB parser, allowing you to utilize Ruby code in the template using the standard ERB[http://ruby-doc.org/stdlib-1.9.3/libdoc/erb/rdoc/ERB.html] template <tt><% %></tt> syntax.  This can be used to peform more complex operations.  If you utilize Ruby code, you will have access to a special local variable called 'manifest_row', which is a hash of values for that row in the manifest, keyed off the column names.  For example:

 <mods><title><%= manifest_row[:description] %></title></mods>
 
will provide the same output as the previous example.  A full example of a MODs template is provided at config/projects/manifest_template/TEMPLATE_mods.xml

To use a different character encoding in your ERB template, put the following at the top of your <em>template.xml</em>:

  <%#-*- coding: UTF-8 -*-%>
  <?xml version="1.0" encoding="UTF-8"?>

== Testing Descriptive Metadata Generation

If you would like to test your MODs template prior to actually accessioning, you can run a "mods report", passing in the YAML config file,
which references your manifest and MODs template, and a writable output folder location.  The report will then generate a MODs file for
each row in your manifest so you can examine the results.  You can limit the number of rows run by temporarily modifying the "limit_n" 
parameter in the YAML file. Note that the output folder MUST exist and must be writable.  Be aware it will become filled with MODs files, one per object.  So if you have a
large number of rows in your manifest, you will end up with many files in your output directory.

ROBOT_ENVIRONMENT=production bundle exec bin/mods_report YAML_CONFIG_FILE OUTPUT_DIRECTORY


== Accession of Specific Objects

For projects with a manifest (e.g. like Revs):

1. Create a new manifest with only the objects you need accessioned.
2. Create a new project config YAML file referencing the new manifest and write to a new progress log file.
3. Run pre-assembly.

For projects that do not use a manifest and which have their objects already registered (e.g. like Rumsey):

1. Create a new project config YAML file and set the parameter 'accession_items' using either the 'only' or
'except' parameter as needed. You can include only specific objects (useful when you only want to run a few
objects) or you can exclude specific objects (useful when you want to run most). Set the 'reaccession'
parameter to false or nil. Also set a different progress log file so you can store the results of your
second run separately. See the TEMPLATE.yaml for some examples.

2.  Run pre-assembly.

== Re-Accession of Specific Objects

Very similar to above, if you need to re-accession a batch of material (for example, after remediating some
files in your bundle), you can do this in two ways, depending on your project setup.

For projects with a manifest (e.g. like Revs):

1. Create a new manifest with only the objects you need re-accessioned. 
2. Create a new project config YAML file referencing the new manifest and write to a new progress log file. 
3. "Cleanup" your existing objects that you will be re-accessioning using the "Assembly::Utils.cleanup" method on a Ruby console as described below. Since you will be re-registering objects, you will get new DRUIDs, and you should therefore be sure to completely delete your old objects. 
4. Re-run pre-assembly.

For projects that do not use a manifest and which have their objects already registered (e.g. like Rumsey):

1. Create a new project config YAML file and set the parameter 'accession_items' and the 'only' parameter to an array of bundle names (e.g. druid folder names) that you want to re-accession.  Set the 'reaccession' parameter to true.  Also set a different progress log file so you can store the results of your second run separately.  See the TEMPLATE.yaml for some examples.
2.  Re-run pre-assembly.

This process will perform an automatic cleanup on the items being re-accessioned (but will leave your
objects registered).

==Cleanup

=== Removing Items From DOR and other locations

If you need to cleanup accessioned content, you can do this in two ways. The first will use the YAML
configuration file and the associated progress log file to indicate which objects you would like removed.
Note that the YAML configuration file specifies the location of the progress log file. If the progress log
file has been moved or changed, this will not work correctly. You should confirm that your YAML
configuration file still correctly specifies the location of "progress_log_file". The script will open the
YAML configuration, find the progress log, read it to find the actual completed objects, and will then
execute cleanup on those objects. Use the following script to perform a cleanup:

    ROBOT_ENVIRONMENT=xxx bin/cleanup YAML_PROGRESS_LOG_FILE steps
    
where steps is a comma delimited list of any or all of the following steps defined below:

symlinks:: remove symlinks from the /dor/workspace
stage:: removed staged files (typically stored in /dor/assembly, but the specific area is defined as 'staging_dir' in the YAML file)
dor:: remove objects from Fedora
stacks:: remove files from the stacks that were shelved during accessioning ... note this step must be run from a server (such as sul-lyberservices-test) and you must be able to authenticate to the relevant stacks server
workflows:: remove the assemblyWF and accessionWF workflows for the objects

You can also specify the environment using ROBOT_ENVIRONMENT, just as with a pre-assembly run. Since this
script is destructive, you will need to confirm each step. It is always best to run this script on the test
(or production) server, since then it will have full access to the stacks.

For example, when on sul-lyberservices-test:

 ROBOT_ENVIRONMENT=test bin/cleanup tmp/revs.yaml symlinks,stage,dor

The second way to perform a cleanup is from a bin/console prompt running the appropriate environment. You
can then specify a specific list of druids and steps (as an array of symbols) to cleanup:

e.g. 

 $ ROBOT_ENVIRONMENT=test bin/console

 druids=%w{druid:aa111aa1111 druid:bb222bb2222}
 steps=[:symlinks,:stage,:dor,:stacks,:workflows]
 Assembly::Utils.cleanup(:druids=>druids,:steps=>steps,:dry_run=>true)

=== Loading YAML Configuration

If you are working in the console, and want to read your YAML configuration file (for example, to determine
where your progress log file is located), you can use the following methods to load the configuration into a
ruby hash:

e.g. 

 $ ROBOT_ENVIRONMENT=test bin/console

 config_filename='/thumpers/dpgthumper2-smpl/SC1017_SOHP/sohp_prod_accession.yaml'
 config=Assembly::Utils.load_config(config_filename)
 progress_filename=config['progress_log_file']

You can then use these values in other utility methods as needed.

=== Finding Druids 

If you want to find the druids from your progress log file that are either completed or not completed, you
can use a method that will give you an array of relevant druids. You can then use this array in
'workflow_status' method noted above or in the other utility methods.

e.g. 

 $ ROBOT_ENVIRONMENT=test bin/console

 completed_druids=Assembly::Utils.get_druids_from_log('/dor/preassembly/sohp_accession_log.yaml',true)
 failed_druids=Assembly::Utils.get_druids_from_log('/dor/preassembly/sohp_accession_log.yaml',false)

 # e.g. get workflow status of failed druids:
 Assembly::Utils.workflow_status(:druids=>failed_druids,:workflows=>[:assembly,:accession],:filename=>'output.csv')

If you want to find druids by source_id, use the utility method
Assembly::Utils.get_druids_by_sourceid(source_ids=[]) to do this. You can then use the array of druids in
the other utility methods.

e.g. 

 $ ROBOT_ENVIRONMENT=test bin/console

 source_ids=%w{foo:123 bar:456}
 druids=Assembly::Utils.get_druids_by_sourceid(source_ids)
 Assembly::Utils.workflow_status(:druids=>druids,:workflows=>[:assembly,:accession],:filename=>'output.csv')

=== Workflow Status Report

If you want to check on the status of objects in DOR without using Argo, you can do this using the following
method. You can specify assembly and/or accession workflows (as symbols), and also a filename if you want
the report written to disk in CSV format.

 $ ROBOT_ENVIRONMENT=test bin/console

 druids=%w{druid:aa111aa1111 druid:bb222bb2222}
 Assembly::Utils.workflow_status(:druids=>druids,:workflows=>[:assembly,:accession])  # add :filename=>'output.csv' to get a CSV report

=== Updating Datastreams

Once you have content accessioned, you might need to batch update datastreams (for example, to globally fix
a typo). You can do this by providing an array of druids, the name of a datastream and some new content:

 druids=%w{druid:aa111aa1111 druid:bb222bb2222}
 new_content='<xml><more nodes>this should be the whole datastream</more nodes></xml>'
 datastream='rightsMetadata'
 Assembly::Utils.replace_datastreams(druids,datastream,new_content)

You can also just replace a part of a datastream instead of the whole thing. Be careful, this just runs a
.gsub on the entire datastream, so it can do damage if you aren't careful.

 druids=%w{druid:aa111aa1111 druid:bb222bb2222}
 find_content='FooBarBaz'
 replace_content='Stanford Rules'
 datastream='rightsMetadata'
 Assembly::Utils.update_datastreams(druids,datastream,find_content,replace_content)

A helper method is provided to update rightsMetadata using the default rights contained in an APO. To do
this, supply a list of druids to update and the druid of the APO to get default rights from:

 druids=%w{druid:aa111aa1111 druid:bb222bb2222}
 apo_druid='druid:cc222cc2222'
 Assembly::Utils.update_rights_metadata(druids,apo_druid)

=== Finding Errored Out Objects

You can get a hash containing objects that have errored out in a specific workflow and step, including the
error message. Note that if you don't supply a tag, both of the following commands work for *ALL* objects in
DOR, they are not specific to your project. If you do supply a tag, it should be exact (e.g. 'Project :
Revs'), and if you have a lot of objects in DOR in a specific error state, the call may take a long time,
since it will need to look up each object in DOR.

 $ ROBOT_ENVIRONMENT=test bin/console

 result=Assembly::Utils.get_errored_objects_for_workstep('accessionWF','content-metadata')

To filter to a specific project, supply a tag:

 result=Assembly::Utils.get_errored_objects_for_workstep('accessionWF','content-metadata','Project : Revs')

You can also automatically reset all of those objects to waiting:

 result=Assembly::Utils.reset_errored_objects_for_workstep('accessionWF','content-metadata')

You can also supply a tag here:

 result=Assembly::Utils.reset_errored_objects_for_workstep('accessionWF','content-metadata','Project : Revs')

=== Reset Workflow States

If an object fails for some reason and you manually remediate something, you may need to reset the workflow
state for certain steps to try again. You can reset any workflow steps back to "waiting" for any list of
druids you specify and any workflow states. To do this use the Assembly::Utils.reset_workflow_states method.
Provide a list of druids in an array, and a hash containing workflow names (e.g. 'assemblyWF' or
'accessionWF') as the keys, and arrays of steps as the corresponding values (e.g.
['checksum-compute','jp2-create']) and they will all be reset to "waiting".

e.g. 

  $ ROBOT_ENVIRONMENT=test bin/console

  druids=%w{druid:aa111aa1111 druid:bb222bb2222}
  steps={'assemblyWF'  => ['checksum-compute'],'accessionWF' => ['content-metadata','descriptive-metadata']}
  Assembly::Utils.reset_workflow_states(:druids=>druids,:steps=>steps)

== Remediation

=== Basic Object Remediation

A basic object remediation framework is provided for mass remediating objects.  

To use it, first decide how you will specify the list of druids.  You can either manually specify with a CSV (see step 1 below), or
use the pre-assembly YAML log file if you are remediated a run from pre-assembly. 

1. For running a specific list of druids, create a CSV file containing a list of druids you wish to remediate, one line per druid, with a header column called "druid".  
There can be other columns too -- they will be ignored.  These should be full druids, e.g.
"druid:oo000oo0001".  Save it somewhere the pre-assembly code can read it.

2. Create a ruby file that defines exactly how you need to remediate each object.  The ruby file will define a method that has access to the
fedora object and can perform any actions needed on the object.  The actual loading, saving, and versioning of objects is handled by the
framework - you just need to define the actual logic needed to operate on the object.  The file needs to have a specific format with
two specific methods that must be defined, one that determines if remediation will occur at all, and the second indicates the type
of remediation to perform.  An example file and its format is shown in the file 'lib/remediation/remediate_project_example.rb'  
Don't edit that file - copy it, and edit it somewhere the script can read it.

Note that you have access to 'equivalent-xml' (https://github.com/mbklein/equivalent-xml) for comparing xml when deciding if you need to
remediate.  You also can use nokogiri. If you use equivalent-xml be sure to require it at the top of your script.

3. Run with the command below (presumably on the lyberservices-prod server, which has access to Fedora production, although you can also
run in test or development mode if you can access those environments).  Pass in the either the CSV file with DRUIDs or the pre-assembly YAML file
and the Ruby file you generated in steps 1 and 2.  You will get two output files -- the first is a CSV file which includes columns indicating if remediation succeeded, a message
and a timestamp.  You will also get a .log file.  Each file is named in the same way as the input file with _log.csv and _log.yml appended to the
filename and is placed in the same location.  This means you should place the input CSV in be a location where the script will have write access
to that location (i.e. not on a thumper drive if you are on lyberservices-prod).  

Note that the _log.yml file is used to ensure that objects are not run through remediation twice, so you should keep that file in the
same location as the input CSV if you need to resume a large remediation.  In each case, the file will always be appended to.

ROBOT_ENVIRONMENT=production bin/remediate INPUT_FILE.CSV[or LOG_FILE.YAML] REMEDIATE_LOGIC_FILENAME.rb

For long running actions, you can run in nohup mode

ROBOT_ENVIRONMENT=production nohup bin/remediate INPUT_FILE.CSV[or LOG_FILE.YAML] REMEDIATE_LOGIC_FILENAME.rb &

The result will be some screen output and a detailed log file in .YML format in the same location and with the same name as the input CSV file.
The input CSV file will also be updated with two additional columns - a status indicating if remediation succeeded and a message.
You can re-run the remediation with the same CSV file and it will automatically skip already completed objects - SO KEEP THE CSV FILE.

=== Custom Object Remediation 

You can also build a fully customized remediation script that does not require input DRUIDs in a CSV and can pass data to
individual druids for more specific remediation.  To do this, you will need to implement the logging or resume capability you will need.  
You will also still implement your own project specific Ruby file (as in step 2 above) but you will also implement your own mechanism for 
generating druids to remediate and for logging their completion. 

Within the pre-assembly codebase, you will have access to the following remediation class.   Pass it a druid and your remediation logic Ruby
file, and it will return you success or failure and a message.  You can also use the built in logging methods to record success/failure
to help with resuming.  You can also optionally pass any data structure or object you need and have that available in your custom method.

 project_file='my_remediation.rb'
 pid='druid:oo000oo0001'
 require project_file

 item=PreAssembly::Remediation::Item.new(pid,optionalDataStructure) # optionalDataStructure is some kind of object, string, hash, etc.
                                                                    # it will be made available to your "remediate_logic" method in 
  	                                                                # the instance variable @data
 item.extend(RemediationLogic) # add in our project specific methods
 item.description='some description that will be added to any versions which are opened' # optional
 success=item.remediate # returns true or false
 puts item.message # some more info about what was done

Loop over your PIDs and log results as needed.  You can log to an output CSV and/or a YML file if you wish, using the following methods

 item.log_to_progress_file(progress_log_file)  # pass in a fully qualified path to a YML file to append to
 item.log_to_csv(csv_out)   # pass in a fully qualified path to a CSV file to append to

You can read in completed druids from a progress_log_file you have previously created using a class level method if you wish to 
check if a druid is already completed.  Each of these calls gives you an array of druids.

 completed_druids=PreAssembly::Remediation::Item.get_druids(progress_log_file)
 failed_druids=PreAssembly::Remediation::Item.get_druids(progress_log_file,false)
 done if completed_druids.include?(pid)  # will give you a true or false if your current pid is already done
   
==Assembly Robots

The last step of pre-assembly is the initiation of the assembly workflow for a given object.  This assumes the assemblyWF is defined in the APO referenced by the project, and it also assumes the assembly robots are running on the server.  If the assembly robots are not running, then JP2 creation and all follow up steps will not occur.  You can determine if the robots are running on the server with:

  ps -ef | grep assemblyWF

If you see only one line, something like below, the assembly robots are  NOT running:

  501 26382 26097   0  1:04PM ttys001    0:00.00 grep assemblyWF

If you see multiple lines, something like below, the assembly robots are running:

  503        782     1  0 10:25 ?        00:00:04 assemblyWF:jp2-create
  503        784     1  0 10:25 ?        00:00:04 assemblyWF:checksum-compute
  503        789     1  0 10:25 ?        00:00:07 assemblyWF:exif-collect
  503        791     1  0 10:25 ?        00:00:10 assemblyWF:accessioning-initiate
  503       9009  8909  0 13:05 pts/0    00:00:00 grep assemblyWF

To start the assembly robots, use the following line.  If you are in production substitute "production" for "test" in the robot_environment variable:
  
  cd /home/lyberadmin/assembly/current; ROBOT_ENVIRONMENT=test ./bin/run_robot start assemblyWF:jp2-create assemblyWF:checksum-compute assemblyWF:exif-collect assemblyWF:accessioning-initiate

You can track the progress of each assembly robot by inspecting it's log file on the server.  To see the available log files (one for each robot):
  
  ls /home/lyberadmin/assembly/current/log
  
Tail a file to watch it's output, e.g.
  
  tail -f /home/lyberadmin/assembly/current/log/jp2-create.log

Or inspect it for a more complete history, e.g.
  
  less /home/lyberadmin/assembly/current/log/jp2-create.log

More information on the assembly robots is found in the assembly project here:

  corn.stanford.edu:/afs/ir/dev/dlss/git/lyberteam/assembly.git


    